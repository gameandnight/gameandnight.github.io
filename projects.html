<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Proyectos - Iker Redondo Serra</title>
    <meta name="description" content="Proyectos de ML e IA generativa de Iker Redondo Serra.">
    <!-- Canonical -->
    <link rel="canonical" href="https://gameandnight.github.io/projects.html">
    <!-- Open Graph -->
    <meta property="og:title" content="Proyectos – Iker Redondo Serra">
    <meta property="og:description" content="Descubre los proyectos de ML e IA generativa de Iker Redondo Serra.">
    <meta property="og:url" content="https://gameandnight.github.io/projects.html">
    <meta property="og:image" content="https://gameandnight.github.io/assets/images/proyecto-ejemplo.jpg">
    <meta property="og:type" content="website">
    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Proyectos – Iker Redondo Serra">
    <meta name="twitter:description" content="Descubre los proyectos de ML e IA generativa de Iker Redondo Serra.">
    <meta name="twitter:image" content="https://gameandnight.github.io/assets/images/proyecto-ejemplo.jpg">
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
      .nav-link[aria-current="page"] {
        font-weight: bold;
        color: #0d6efd;
      }
      /* Asegura que todas las imágenes de proyectos tengan la misma altura y cobertura */
      .project-img {
        width: 100%;
        height: 400px; /* Ajusta la altura según convenga */
        object-fit: cover;
      }
      /* Margen inferior entre imágenes apiladas */
      .project-img + .project-img {
        margin-top: 0.5rem;
      }
      /* Opcional: si quieres que la columna de imagen no se encoja demasiado en pantallas pequeñas */
      @media (min-width: 768px) {
        .col-md-4 {
          max-width: 33.3333%;
        }
      }
    </style>
  </head>
  <body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
      <div class="container-fluid">
        <a class="navbar-brand" href="/">Iker Redondo Serra</a>
        <button
          class="navbar-toggler"
          type="button"
          data-bs-toggle="collapse"
          data-bs-target="#navbarNav"
          aria-controls="navbarNav"
          aria-expanded="false"
          aria-label="Toggle navigation"
        >
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ms-auto">
            <li class="nav-item">
              <a class="nav-link" href="/">Inicio</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/about.html">Sobre mí</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/projects.html" aria-current="page">Proyectos</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/contact.html">Contacto</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="/assets/cv/CV.pdf" target="_blank" rel="noopener">Descargar CV</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <section class="bg-light py-5">
      <div class="container">
        <h1>Proyectos Destacados</h1>

        <!-- Proyecto: Detección de señales de tráfico con YOLO11x -->
        <div class="row mb-4 align-items-start">
          <!-- Columna de imagen: se centra verticalmente con align-items-start en la fila -->
          <div class="col-md-4 d-flex flex-column align-items-center">
            <!-- Imagen representativa: capturas de pantalla de la demo en tiempo real -->
            <img
              src="/assets/images/deteccion-senales.jpg"
              alt="Detección de señales de tráfico con YOLO11x"
              class="img-fluid rounded project-img"
            >
            <!-- Comentario: 
                 Aquí podrías incluir varias mini-imágenes (thumbnails) si quieres mostrar pasos intermedios, 
                 p.ej., una captura del etiquetado en CVAT, otra del entrenamiento en consola, otra de la inferencia en pantalla.
                 Ejemplo:
                 <img src="/assets/images/detalle-cvat.jpg" alt="Etiquetado en CVAT" class="img-fluid rounded project-img mt-2">
            -->
          </div>

          <div class="col-md-8">
            <h3>Detección de señales de tráfico con YOLO11x</h3>
            <!-- Breve descripción general -->
            <p>
              Proyecto de fine-tuning de un modelo YOLO11x para detección de señales de tráfico en tiempo real mediante webcam. 
              Se parte de un dataset preexistente de ~12.000 imágenes etiquetadas, reforzando clases menos representadas
              y entrenando localmente con early-stopping y ajustes de hiperparámetros para equilibrar recursos y precisión.
            </p>

            <!-- Pipeline detallado -->
            <p><strong>Pipeline completo:</strong></p>
            <ul>
              <!-- Investigación de herramientas de etiquetado -->
              <li>
                <strong>Investigación de herramientas:</strong> 
                Se evaluaron CVAT y Roboflow. Se seleccionó CVAT por control local de datos y exportación directa a formato YOLO
                (a pesar de limitaciones en auto-etiquetado), permitiendo un flujo de trabajo local seguro.
                <!-- Comentario: podrías añadir enlace a la documentación interna o a ejemplos de configuración Docker de CVAT. -->
              </li>

              <!-- Creación y refuerzo de dataset -->
              <li>
                <strong>Creación y refuerzo del dataset:</strong> 
                Partiendo de un dataset preexistente, se identificaron clases poco representadas (e.g., “giro a la izquierda”) y se añadieron manualmente imágenes etiquetadas en CVAT para balancear.
                Exportación en formato YOLO generó archivos de coordenadas listos para entrenamiento local.
                <!-- Comentario: podrías enlazar al script de procesamiento de data.yaml o al README que explica la estructura de carpetas. -->
              </li>

              <!-- Entrenamiento inicial -->
              <li>
                <strong>Entrenamiento inicial con YOLO11n:</strong> 
                Se entrenó primero en portátil (~24h) y luego en máquina más potente (<4h) con early-stopping antes de 100 épocas, validando rendimiento en clasificación de imágenes. Los nombres de señales se adaptaron a castellano en segundo entrenamiento.
                <!-- Comentario: referencia a “entrenamiento.py” y logs de entrenamiento, ubicado en carpeta “resultados entrenamiento”. -->
              </li>

              <!-- Pruebas en tiempo real con YOLO11n -->
              <li>
                <strong>Pruebas en tiempo real con YOLO11n:</strong> 
                Se integró modelo en script de prueba (prueba.py), capturando con OpenCV y leyendo nombre por síntesis de voz. Se observaron limitaciones: lentitud en inferencia y detecciones ruidosas con muchas clases.
                <!-- Comentario: problemas detectados incluyen congelamiento de cámara durante la síntesis de audio y solapamiento de lecturas. -->
              </li>

              <!-- Migración a YOLO11x -->
              <li>
                <strong>Migración a YOLO11x:</strong> 
                Para mejorar rendimiento en tiempo real, se entrenó un modelo YOLO11x (~13h en PC con GPU, ajustando batch size por VRAM). Early-stopping antes de 100 épocas. Este modelo mostró mejor facilidad para detectar en webcam, aunque aún con falsos positivos debido a gran número de clases.
                <!-- Comentario: puedes enlazar al script específico de entrenamiento con YOLO11x y parámetros usados. -->
              </li>

              <!-- Evaluación y reducción de clases -->
              <li>
                <strong>Evaluación y reducción de clases en inferencia:</strong> 
                En el script de detección en tiempo real se redujo temporalmente el conjunto de clases a detectar (ej. 10 en lugar de 49), mejorando estabilidad y reduciendo falsos positivos.  
                <!-- Comentario: indica dónde en el código se filtran las clases activas. -->
              </li>

              <!-- Ajustes de inferencia y síntesis de voz -->
              <li>
                <strong>Ajustes en prueba en tiempo real:</strong> 
                - Gestión de congelamientos: se pausó la captura de frames antes de reproducir audio y se retomó tras finalizar la lectura.  
                - Manejo de múltiples detecciones simultáneas: lógica para seleccionar lectura de señal prioritaria o en cola, evitando interrupciones mutuas.  
                <!-- Comentario: referencia a fragmento de código donde se controla la espera de audio y reanudación de bucle OpenCV. -->
              </li>

              <!-- Optimización futura -->
              <li>
                <strong>Optimización futura:</strong> 
                - Reforzar dataset manualmente o mediante scripts de reetiquetado para balancear todas las clases y reducir confusiones.  
                - Probar modelos intermedios (YOLO11m, YOLO11l) para equilibrio entre recursos y velocidad.  
                - Data augmentation y ajustes de hiperparámetros (learning rate, image size, batch size) en entrenamiento.  
                - Posible integración de detección multitarea: filtrar clases irrelevantes según contexto o región geográfica.  
                <!-- Comentario: enlazar a scripts de data augmentation o notas de configuración. -->
              </li>

              <!-- Documentación y resultados -->
              <!-- <li>
                <strong>Documentación y resultados:</strong> 
                - Carpeta “resultados entrenamiento” incluye métricas (mAP, curvas de entrenamiento).  
                - Carpeta “pruebas imagenes” con capturas de detección en imágenes estáticas.  
                - Scripts “entrenamiento.py” y “prueba.py” aportan reproducibilidad.  -->
                <!-- Comentario: se puede agregar enlace o referencia al README en el repositorio que explique la estructura de carpetas del proyecto. -->
              <!-- </li> -->
            </ul>

            <!-- Enlaces de interés -->
            <p>
              <!-- Descomenta según disponibilidad: -->
              <!-- Enlace al repositorio de código: reemplaza con tu URL real -->
              <!-- <a href="https://github.com/gameandnight/deteccion-senales" class="btn btn-primary btn-sm" target="_blank" rel="noopener">Código</a> -->
              <!-- Si existe demo online o notebook con prueba, añade enlace aquí -->
              <!-- <a href="https://tu-demo.streamlitapp.com" class="btn btn-secondary btn-sm" target="_blank" rel="noopener">Demo</a> -->
              <!-- Enlace a vídeo de demostración en YouTube mostrando el flujo completo -->
              <!-- <a href="https://www.youtube.com/watch?v=TU_VIDEO_ID" class="btn btn-info btn-sm" target="_blank" rel="noopener">Ver Vídeo</a> -->
            </p>

            <!-- Opcional: embed de vídeo de demo -->
            <!--
            <div class="mt-3">
              <div class="ratio ratio-16x9">
                <iframe src="https://www.youtube.com/embed/TU_VIDEO_ID" title="Demo detección señales" allowfullscreen></iframe>
              </div>
            </div>
            -->
          </div>
        </div>

        <!-- Proyecto: Suite de Mini Juegos controlados con gestos (OpenCV + Tkinter) -->
        <div class="row mb-4 align-items-start">
          <div class="col-md-4 d-flex flex-column align-items-center">
            <img
              src="/assets/images/mini-juegos-gestos.jpg"
              alt="Suite de Mini Juegos controlados con gestos"
              class="img-fluid rounded project-img"
            >
          </div>
          <div class="col-md-8">
            <h3>Suite de Mini Juegos controlados con gestos (OpenCV + Tkinter)</h3>
            <p>Aplicación en Python que ofrece varios mini juegos interactivos controlados mediante gestos o pose corporal, usando OpenCV y una interfaz Tkinter.</p>
            <p><strong>Descripción general / Pipeline:</strong></p>
            <ul>
              <li>Launcher en Tkinter con botones para cada mini juego: "Manos", “Cuerpo”, “Dibujar”, “Atrapa la pelota”, “Piedra, Papel o Tijera” y “Snake”.</li>
              <li>Para detección de landmarks de mano y pose se emplea CVZone PoseModule/HandTracking, facilitando el uso de MediaPipe para extraer coordenadas clave.</li>
              <li>Lógica de cada mini juego: captura de cámara con <code>cv2.VideoCapture</code>, procesamiento de frames, interpretación de gestos para mover sprites, dibujar o decidir jugadas.</li>
              <li>Manejo de eventos de teclado (‘q’ para salir) y retorno al menú sin cerrar la app por completo.</li>
              <!-- <li>Pruebas y calibraciones para asegurar suficiente FPS y detección estable en distintas condiciones de iluminación.</li> -->
            </ul>
            <p>
              <!-- <a href="https://github.com/gameandnight/mini-juegos-gestos" class="btn btn-primary btn-sm" target="_blank" rel="noopener">Código</a> -->
              <!-- <a href="https://tu-demo-mini-juegos.streamlitapp.com" class="btn btn-secondary btn-sm" target="_blank" rel="noopener">Demo</a> -->
              <!-- <a href="https://www.youtube.com/watch?v=ID_VIDEO_MINIJUEGOS" class="btn btn-info btn-sm" target="_blank" rel="noopener">Ver Vídeo</a> -->
            </p>
            <!-- Opcional embed:
            <div class="mt-3">
              <div class="ratio ratio-16x9">
                <iframe src="https://www.youtube.com/embed/ID_VIDEO_MINIJUEGOS" title="Demo Mini Juegos controlados con gestos" allowfullscreen></iframe>
              </div>
            </div>
            -->
          </div>
        </div>

        <!-- Proyecto: Juego de Trivial controlado con gestos (OpenCV + CVZone + Tkinter) -->
        <div class="row mb-4 align-items-start">
          <div class="col-md-4 d-flex flex-column align-items-center">
            <!-- Múltiples imágenes apiladas verticalmente -->
            <img
              src="/assets/images/trivial-gestos.jpg"
              alt="Juego de Trivial - captura 1"
              class="img-fluid rounded project-img"
            >
            <!-- <img
              src="/assets/images/trivial-gestos-2.jpg"
              alt="Juego de Trivial - captura 2"
              class="img-fluid rounded project-img"
            > -->
            <!-- Puedes añadir más imágenes si deseas, con la misma clase .project-img -->
          </div>
          <div class="col-md-8">
            <h3>Juego de Trivial controlado con gestos (OpenCV + CVZone + Tkinter)</h3>
            <p>Aplicación en Python que presenta preguntas de trivia en diferentes categorías, permitiendo seleccionar la respuesta mediante gestos de mano (rock/paper/scissors) capturados por la webcam.</p>
            <p><strong>Descripción general / Pipeline:</strong></p>
            <ul>
              <li><strong>Interfaz de menú con Tkinter:</strong> Al iniciar, se muestra una ventana Tkinter con botones para cada categoría de preguntas (p. ej. ESO, Bachiller, Informática). El usuario selecciona una categoría para iniciar el trivial.</li>
              <li><strong>Carga de preguntas:</strong> El código lee archivos JSON con preguntas y opciones (por ejemplo <code>questions_eso.json</code>, <code>questions_bachiller.json</code>), donde cada elemento incluye la pregunta, las opciones y el índice de la respuesta correcta.</li>
              <li><strong>Preparación de recursos visuales:</strong> Se carga una imagen de fondo (<code>Resources/BG.png</code>) redimensionada a la resolución de la ventana (p.ej. 1280×720). Se preparan imágenes de feedback (correcto/incorrecto) (<code>correct.wav</code>, <code>incorrect.wav</code>) y sprites opcionales.</li>
              <li><strong>Captura de cámara y detección de gestos:</strong>
                <ul>
                  <li>Se inicializa <code>cv2.VideoCapture(0)</code> para capturar frames de la webcam.</li>
                  <li>Se utiliza CVZone HandDetector (basado en MediaPipe) para detectar la mano y extraer landmarks, con los cuales se interpreta el gesto rock/paper/scissors como selección de respuesta. Esto permite mapear cada opción a un gesto: “Piedra”, “Papel” o “Tijeras”.</li>
                  <li>En cada pregunta, tras mostrarla en pantalla, se espera a que el detector identifique claramente uno de los gestos. Durante esta espera, se muestra la imagen de fondo más un recuadro con el feed de cámara, indicando al usuario que haga el gesto.</li>
                </ul>
              </li>
              <li><strong>Síntesis de voz y feedback:</strong>
                <ul>
                  <li>Se usa gTTS para convertir a audio el texto de la pregunta y de las opciones, generando un archivo de audio temporal que se reproduce (por ejemplo con pygame) para narrar la pregunta y las opciones.</li>
                  <li>Tras que el usuario hace el gesto y se detecta la elección, se compara con la respuesta correcta almacenada en JSON.</li>
                  <li>Se reproduce un sonido o mensaje de audio indicando “Correcto” o “Incorrecto” (<code>Resources/correct.wav</code> / <code>Resources/incorrect.wav</code>).</li>
                </ul>
              </li>
              <li><strong>Lógica de flujo de juego:</strong>
                <ul>
                  <li>Se muestra la pregunta y se invoca la narración por voz.</li>
                  <!-- <li>Se espera la selección por gesto; si no se detecta tras cierto tiempo, se puede repetir la pregunta o avisar al usuario.</li> -->
                  <li>Tras respuesta, se muestra feedback y pasa a la siguiente pregunta o finaliza la ronda.</li>
                  <li>Al finalizar la categoría, se cierra la ventana de OpenCV y se regresa al menú Tkinter.</li>
                </ul>
              </li>
              <!-- <li><strong>Configuración y calibración:</strong>
                <ul>
                  <li>Permite ajustar la confianza del detector de manos y la sensibilidad de reconocimiento de gestos para funcionar en distintas condiciones de iluminación.</li>
                  <li>Verifica FPS para asegurar experiencia fluida. Si la velocidad de detección es baja, se puede reducir la resolución o ajustar parámetros del detector.</li>
                </ul>
              </li> -->
              <li><strong>Gestión de archivos:</strong>
                <ul>
                  <li>Archivos JSON en carpeta raíz o en <code>Resources/</code>, por ejemplo <code>questions_eso.json</code>, <code>questions_bachiller.json</code>, <code>questions_profesores.json</code>.</li>
                  <li>Recursos en <code>Resources/</code>: <code>BG.png</code>, <code>correct.wav</code>, <code>incorrect.wav</code>, y otros assets.</li>
                </ul>
              </li>
              <li><strong>Dependencias:</strong>
                <ul>
                  <li>Python 3.x</li>
                  <li>OpenCV (<code>opencv-python</code>)</li>
                  <li>CVZone (<code>cvzone</code>, <code>mediapipe</code>)</li>
                  <li>gTTS (<code>gtts</code>), pygame (para reproducir audio) o librería similar</li>
                  <li>Pillow (<code>PIL</code>) para dibujar texto sobre la imagen con fuentes personalizadas</li>
                  <li>Tkinter (incluido en la mayoría de distribuciones Python) para la GUI del menú</li>
                  <li>Otros: <code>numpy</code>, <code>random</code>, <code>time</code>, <code>json</code> etc.</li>
                </ul>
              </li>
              <!-- <li><strong>Pruebas y grabación de demo:</strong>
                <ul>
                  <li>Grabar con OBS o Game Bar mostrando la ventana Tkinter de selección y luego la sesión de trivial: pregunta en pantalla, usuario realiza gesto, detección y feedback de voz.</li>
                  <li>Tomar capturas de pantalla para la imagen representativa del proyecto en tu portfolio.</li>
                </ul>
              </li> -->
            </ul>
            <p>
              <!-- Enlace al repositorio de código: reemplaza con tu URL real -->
              <!-- <a href="https://github.com/gameandnight/trivia-gestos" class="btn btn-primary btn-sm" target="_blank" rel="noopener">Código</a> -->
              <!-- Si tuvieras demo online, descomenta y ajusta este enlace -->
              <!-- <a href="https://tu-demo-trivia.streamlitapp.com" class="btn btn-secondary btn-sm" target="_blank" rel="noopener">Demo</a> -->
              <!-- Enlace a vídeo de demostración en YouTube mostrando el flujo completo del trivial controlado con gestos -->
              <!-- <a href="https://www.youtube.com/watch?v=ID_VIDEO_TRIVIAL" class="btn btn-info btn-sm" target="_blank" rel="noopener">Ver Vídeo</a> -->
            </p>
            <!-- Opcional embed de vídeo:
            <div class="mt-3">
              <div class="ratio ratio-16x9">
                <iframe src="https://www.youtube.com/embed/ID_VIDEO_TRIVIAL" title="Demo Juego de Trivial con gestos" allowfullscreen></iframe>
              </div>
            </div>
            -->
          </div>
        </div>

        <!-- Proyecto: Juego de Plataformas y Shooter de Naves controlado con gestos RPS -->
        <div class="row mb-4 align-items-start">
          <div class="col-md-4 d-flex flex-column align-items-center">
            <!-- Múltiples imágenes apiladas verticalmente -->
            <img
              src="/assets/images/juego-nivel1.jpg"
              alt="Juego Plataformas - Nivel 1"
              class="img-fluid rounded project-img"
            >
            <img
              src="/assets/images/juego-nivel2.jpg"
              alt="Juego Plataformas - Nivel 2"
              class="img-fluid rounded project-img"
            >
            <!-- <img
              src="/assets/images/juego-nivel3.jpg"
              alt="Juego Shooter de Naves - Nivel 3"
              class="img-fluid rounded project-img"
            > -->
            <!-- Si deseas más capturas, añade más <img> con la clase project-img -->
          </div>
          <div class="col-md-8">
            <h3>Juego de Plataformas y Shooter de Naves controlado con gestos RPS</h3>
            <p>Aplicación en Python que combina dos niveles de plataformas y un tercer nivel de shooter de naves con scroll automático y boss final, controlados mediante gestos de mano (Rock/Paper/Scissors) detectados con la webcam.</p>
            <p><strong>Descripción general / Pipeline:</strong></p>
            <ul>
              <li><strong>Estructura modular:</strong>
                <ul>
                  <li><code>game.py</code>: Punto de entrada. Inicializa Pygame, HandDetector (CVZone) y controla el bucle principal, alternando entre modos de plataforma y modo shooter según selección.</li>
                  <li><code>gestures.py</code>: Función <code>detect_gesture</code> que recibe lista de manos detectadas por <code>HandDetector</code> y devuelve “piedra”, “papel” o “tijeras” según dedos levantados (rock/paper/scissors). Mapea cada gesto a acciones del juego.</li>
                  <li><code>modePlatform.py</code>: Lógica de niveles de plataformas. Obtiene datos de cada nivel desde <code>levels.py</code>, usa <code>pantalla.py</code> para dibujar fondos, bloques, obstáculos, nubes, árboles y UI, y <code>player.py</code> para la clase Player con física (gravedad, salto, colisiones).</li>
                  <li><code>modeSpace.py</code>: Lógica del shooter de naves. Usa <code>spaceship.py</code> para la clase de la nave del jugador, <code>pantalla.py</code> para dibujar fondo espacial, obstáculos y UI, y controla enemigos, disparos y boss final.</li>
                  <li><code>pantalla.py</code>: Funciones para dibujar la interfaz común: parallax backgrounds, UI (vidas, puntuación), obstáculos, sprites, textos en pantalla, gestionando carga única de assets para optimizar rendimiento.</li>
                  <li><code>player.py</code> y <code>spaceship.py</code>: Definen las clases de entidad del jugador: atributos como posición, velocidad, animaciones o sprites, métodos de actualización de posición, colisiones y dibujo en pantalla.</li>
                  <li><code>levels.py</code>: Definición de cada nivel de plataforma (coordenadas de bloques, obstáculos, nubes, trees, enemigos) y funciones para colisiones y generación de contenido procedimental o estático según nivel. También patrones para shooter (enemigos, spawn, boss).</li>
                  <li><code>sound.py</code>: Inicialización y reproducción de música de fondo y efectos de sonido (salto, disparo, explosiones, boss defeat), usando <code>pygame.mixer</code> u otra librería de audio.</li>
                  <li><code>assets/</code>: Carpeta con imágenes y sonidos (fondos, sprites, gestos, efectos). Ejemplos:
                    <ul>
                      <li><code>assets/images/juego-nivel1.jpg</code>, <code>juego-nivel2.jpg</code>, <code>juego-nivel3.jpg</code> para capturas de cada nivel.</li>
                      <li><code>assets/gestos/Rock.png</code>, <code>Paper.png</code>, <code>Scissors.png</code> para feedback de detección de gesto.</li>
                      <li><code>assets/sounds/jump.wav</code>, <code>shoot.wav</code>, <code>explosion.wav</code>, <code>boss.wav</code>, etc.</li>
                    </ul>
                  </li>
                </ul>
              </li>
              <!-- <li><strong>Interfaz de inicio / menú:</strong>
                <ul>
                  <li>Ventana inicial (Tkinter o Pygame): muestra opciones “Nivel 1 (Plataformas)”, “Nivel 2 (Plataformas avanzado)”, “Nivel 3 (Shooter de naves)”.</li>
                  <li>Al elegir nivel, se inicia el modo correspondiente en <code>game.py</code>, que llama a <code>modePlatform.modePlatform(...)</code> o <code>modeSpace.modeSpace(...)</code> con parámetros de configuración (resolución, velocidad, assets cargados, detector de gestos).</li>
                </ul>
              </li> -->
              <li><strong>Captura de cámara y detección de gestos:</strong>
                <ul>
                  <li>Se inicializa <code>cv2.VideoCapture(0)</code> para leer frames de la webcam.</li>
                  <li>Se crea <code>HandDetector(detectionCon=0.8, maxHands=1)</code> de CVZone para detectar mano y extraer landmarks de MediaPipe con pocas líneas de código.</li>
                  <li>En cada frame, <code>hands, frame = detector.findHands(frame)</code> devuelve lista de manos detectadas y dibuja recuadros sobre la imagen. Luego <code>detect_gesture(hands, detector)</code> evalúa <code>detector.fingersUp(hand)</code> y devuelve:
                    <ul>
                      <li><strong>“piedra”</strong>: en niveles de plataforma se interpreta como “avanzar” o “moverse adelante”; en shooter, se interpreta como “subir”.</li>
                      <li><strong>“papel”</strong>: en plataformas se mapea a “saltar” (salto normal o más alto en nivel 2); en shooter, mapea a “disparar” o activar arma principal.</li>
                      <li><strong>“tijeras”</strong>: en plataformas se mapea a “retroceder” ; en shooter, mapea a “bajar”.</li>
                    </ul>
                  </li>
                  <li>En pantalla de juego, se muestra un recuadro pequeño con feed de cámara y/o íconos de gesto para guiar al usuario y confirmar gesto detectado.</li>
                </ul>
              </li>
              <li><strong>Niveles de Plataformas (Level 1 y Level 2):</strong>
                <ul>
                  <li>Datos de nivel cargados desde <code>levels.py</code>: arrays de bloques, obstáculos y enemigos con patrones de movimiento. Level 2 añade complejidad: plataformas móviles, enemigos con patrones de movimiento.</li>
                  <li>Clase <code>Player</code> gestiona posición, velocidad vertical (gravedad), colisiones con bloques y enemigos. Según gesto:
                    <ul>
                      <li>“piedra”: mueve al jugador hacia la derecha.</li>
                      <li>“papel”: activa salto.</li>
                      <li>“tijeras”: mueve al jugador hacia la izquierda.</li>
                    </ul>
                  </li>
                  <li>En cada iteración de <code>modePlatform</code>:
                    <ul>
                      <li>Actualizar posición del jugador y scroll para simular avance.</li>
                      <li>Dibujo de fondo parallax, bloques, obstáculos, elementos con funciones de <code>pantalla.py</code>.</li>
                      <li>Detección de colisiones y gestión de vida o punto de control.</li>
                      <li>Reproducción de música y efectos con <code>sound.py</code>.</li>
                      <li>Al completar el nivel: mostrar “Nivel completado” y esperar entrada para avanzar al siguiente nivel.</li>
                      <li>Si vida llega a cero: “Game Over” y opción de reiniciar el nivel.</li>
                    </ul>
                  </li>
                  <li>Configuraciones de dificultad: nivel 2  enemigos, saltos más altos y plataformas móviles.</li>
                </ul>
              </li>
              <li><strong>Nivel Shooter de Naves (Level 3):</strong>
                <ul>
                  <li>Fondo espacial dibujado con <code>pantalla.draw_parallax_background</code> usando imágenes en <code>assets/backgrounds/espacio.jpg</code>.</li>
                  <li>Clase <code>Spaceship</code>: posición, animaciones, vida y disparos.</li>
                  <li>Mapeo de gestos:
                    <ul>
                      <li>“piedra”: mover nave (subir).</li>
                      <li>“papel”: disparo principal (crear proyectil con efecto y sonido).</li>
                      <li>“tijeras”:  mover nave (bajar).</li>
                    </ul>
                  </li>
                  <li>Generación de enemigos y obstáculos con patrones en JSON o <code>levels.py</code>, scroll automático del nivel.</li>
                  <li>Boss final:
                    <ul>
                      <li>Aparece sprite de boss con movimiento vertical automático  y que nos dispara cada cierto tiempo.</li>
                      <li>Gestión de vida y UI: mostrar barras de vida, reproducir efectos de  victoria.</li>
                    </ul>
                  </li>
                  <li>Colisiones: disparos vs enemigos/boss y nave vs disparos enemigos, gestionando vida.</li>
                  <li>Al derrotar al boss: mostrar “Jefe derrotado”, reproducir música de victoria y esperar input para volver al primer nivel.</li>
                </ul>
              </li>
              <li><strong>Detección de teclado / gestos adicionales:</strong>
                <ul>
                  <li>‘q’ para salir de cualquier modo y salir del juego.</li>
                  <!-- <li>Gesto especial opcional para pausar/reiniciar.</li> -->
                </ul>
              </li>
              <li><strong>Retroalimentación visual y audio:</strong>
                <ul>
                  <!-- <li>Dibujo de texto en pantalla (ej. “Salto”, “Disparo”, “Vida: X”) con <code>cv2.putText</code> o Pygame <code>font.render</code>.</li> -->
                  <li>Reproducción de sonidos con <code>pygame.mixer</code> (salto, disparo, colisión, música de fondo).</li>
                  <!-- <li>Opcional síntesis de voz (gTTS) para narrar eventos clave (“Nivel completado”, “Jefe derrotado”).</li> -->
                </ul>
              </li>
              <li><strong>Calibración y optimización:</strong>
                <ul>
                  <!-- <li>Ajuste de parámetros de <code>HandDetector</code> (<code>detectionCon</code>, <code>maxHands</code>, tolerancias de <code>fingersUp</code>) para robustez en distintas condiciones de iluminación.</li>
                  <li>Reducir resolución de captura si FPS baja (p.ej. 640×480 en vez de 1280×720).</li> -->
                  <li>Cache de assets en <code>pantalla.py</code> para evitar recargas múltiples de imágenes o sonidos.</li>
                </ul>
              </li>
              <li><strong>Recursos y gestión de archivos:</strong>
                <ul>
                  <li>Carpeta <code>assets/backgrounds/</code>: fondos de plataformas y espacio.</li>
                  <li>Carpeta <code>assets/sprites/</code>: imágenes de jugador, enemigos, naves, obstáculos y para feedback visual de gesto.</li>
                  <!-- <li>Carpeta <code>assets/gestos/</code>: <code>Rock.png</code>, <code>Paper.png</code>, <code>Scissors.png</code> para feedback visual de gesto.</li> -->
                  <li>Carpeta <code>assets/sounds/</code>: <code>jump.wav</code>, <code>shoot.wav</code>, <code>coision.wav</code> etc.</li>
                  <li>Archivos de configuración (JSON/CSV) para patrones de enemigos o parámetros de niveles.</li>
                </ul>
              </li>
              <li><strong>Dependencias:</strong>
                <ul>
                  <li>Python 3.x</li>
                  <li>OpenCV (<code>opencv-python</code>) y CVZone (<code>cvzone</code>, <code>mediapipe</code>) para detección de gestos.</li>
                  <li>Pygame para renderizado 2D, gestión de sonido y detección de teclado.</li>
                  <li>Tkinter (opcional) para menú inicial, o todo en Pygame.</li>
                  <!-- <li>gTTS o librería de audio para síntesis de voz (opcional).</li> -->
                  <!-- <li>Pillow (<code>PIL</code>) para manipular sprites o fondos si es necesario.</li> -->
                  <li>Otras librerías: <code>numpy</code>, <code>random</code>, <code>time</code>, <code>json</code>.</li>
                </ul>
              </li>
              <!-- <li><strong>Pruebas y grabación de demo:</strong>
                <ul>
                  <li>Ejecutar localmente en distintas condiciones de iluminación y ángulos de cámara.</li>
                  <li>Grabar con OBS Studio o Game Bar: mostrar menú inicial, Nivel 1, Nivel 2, Nivel 3 con boss final, evidenciando gestos RPS controlando acciones.</li>
                  <li>Tomar capturas de pantalla representativas de cada nivel para la imagen en el portfolio.</li>
                  <li>Opcionalmente editar vídeo (cortar inicio/fin, añadir texto) antes de subir a YouTube.</li>
                </ul>
              </li> -->
            </ul>
            <p>
              <!-- Enlace al repositorio de código: reemplaza con tu URL real -->
              <!-- <a href="https://github.com/gameandnight/juego-plataformas-naves-gestos" class="btn btn-primary btn-sm" target="_blank" rel="noopener">Código</a> -->
              <!-- Si tuvieras demo online, descomenta y ajusta este enlace -->
              <!-- <a href="https://tu-demo-juego-gestos.streamlitapp.com" class="btn btn-secondary btn-sm" target="_blank" rel="noopener">Demo</a> -->
              <!-- Enlace a vídeo de demostración en YouTube mostrando los tres niveles y boss final controlados con gestos -->
              <a href="https://youtu.be/6NqIDnq6rIg" class="btn btn-info btn-sm" target="_blank" rel="noopener">Ver Vídeo</a>
            </p>
            <!-- Opcional embed de vídeo:
            <div class="mt-3">
              <div class="ratio ratio-16x9">
                <iframe src="https://www.youtube.com/embed/ID_VIDEO_JUEGO_PLATAFORMAS" title="Demo Juego Plataformas y Naves con gestos" allowfullscreen></iframe>
              </div>
            </div>
            -->
          </div>
        </div>
      </div>
    </section>

    <footer class="bg-dark text-white text-center py-3">
      <div class="container"><small>&copy; 2025 Iker Redondo Serra</small></div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
